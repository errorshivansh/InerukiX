#Copyright(C)2021TheHamkerCat&errorshivansh

#PortedsomepartsFromWilliamButcherBot.
#PokedexInlineCreditRed-Aura[Madepranav]
#CreditsGoestoWilliamButcherBot


#SomePartsPortedfromhttps://github.com/TheHamkerCat/WilliamButcherBot
"""
MITLicense
Copyright(c)2021TheHamkerCat
Permissionisherebygranted,freeofcharge,toanypersonobtainingacopy
ofthissoftwareandassociateddocumentationfiles(the"Software"),todeal
intheSoftwarewithoutrestriction,includingwithoutlimitationtherights
touse,copy,modify,merge,publish,distribute,sublicense,and/orsell
copiesoftheSoftware,andtopermitpersonstowhomtheSoftwareis
furnishedtodoso,subjecttothefollowingconditions:
Theabovecopyrightnoticeandthispermissionnoticeshallbeincludedinall
copiesorsubstantialportionsoftheSoftware.
THESOFTWAREISPROVIDED"ASIS",WITHOUTWARRANTYOFANYKIND,EPRESSOR
IMPLIED,INCLUDINGBUTNOTLIMITEDTOTHEWARRANTIESOFMERCHANTABILITY,
FITNESSFORAPARTICULARPURPOSEANDNONINFRINGEMENT.INNOEVENTSHALLTHE
AUTHORSORCOPYRIGHTHOLDERSBELIABLEFORANYCLAIM,DAMAGESOROTHER
LIABILITY,WHETHERINANACTIONOFCONTRACT,TORTOROTHERWISE,ARISINGFROM,
OUTOFORINCONNECTIONWITHTHESOFTWAREORTHEUSEOROTHERDEALINGSINTHE
SOFTWARE.
"""


importdatetime
importre
importtime
importurllib.request
fromdatetimeimportdatetime
fromtypingimportList

importaiohttp
importrequests
frombs4importBeautifulSoup
fromcountryinfoimportCountryInfo
fromfakerimportFaker
fromfaker.providersimportinternet
fromPyDictionaryimportPyDictionary
frompyrogramimporterrors,filters
frompyrogram.typesimport(
InlineKeyboardButton,
InlineKeyboardMarkup,
InlineQueryResultArticle,
InlineQueryResultPhoto,
InputTextMessageContent,
)
fromsearch_engine_parserimportGoogleSearch
fromtswiftimportSong
fromyoutubesearchpythonimportVideosSearch

fromIneruki.configimportget_str_key
fromIneruki.function.inlinehelperimport*
fromIneruki.function.pluginhelpersimportfetch,json_prettify
fromIneruki.services.pyrogramimportpbotasapp

OPENWEATHERMAP_ID=get_str_key("OPENWEATHERMAP_ID","")
TIME_API_KEY=get_str_key("TIME_API_KEY",required=False)

dictionary=PyDictionary()


classAioHttp:
@staticmethod
asyncdefget_json(link):
asyncwithaiohttp.ClientSession()assession:
asyncwithsession.get(link)asresp:
returnawaitresp.json()

@staticmethod
asyncdefget_text(link):
asyncwithaiohttp.ClientSession()assession:
asyncwithsession.get(link)asresp:
returnawaitresp.text()

@staticmethod
asyncdefget_raw(link):
asyncwithaiohttp.ClientSession()assession:
asyncwithsession.get(link)asresp:
returnawaitresp.read()


__mod_name__="Inline"
__help__="""
<b>INLINEBOTSERVICEOF@INERUKIBOT</b>

<i>I'mmoreefficientwhenaddedasgroupadmin.Bythewaythesecommandscanbeusedbyanyoneinagroupviainline.</i>

<b>Syntax</b>
@InerukiBot[command][query]

<b>CommandsAvailable</b>
-alive-CheckBot'sStats.
-yt[query]-YoutubeSearch.
-tr[LANGUAGE_CODE][QUERY]**-TranslateText.
-modapk[name]-Giveyoudirectlinkofmodapk.
-ud[QUERY]-UrbanDictionaryQuery
-google[QUERY]-GoogleSearch.
-webss[URL]-TakeScreenshotOfAWebsite.
-bitly[URL]-ShortenALink.
-wall[Query]-FindWallpapers.
-pic[Query]-Findpictures.
-saavn[SONG_NAME]-GetSongsFromSaavn.
-deezer[SONG_NAME]-GetSongsFromDeezer.
-torrent[QUERY]-TorrentSearch.
-reddit[QUERY]-Getmemesfromreddit.
-imdb[QUERY]-Searchmoviesonimdb.
-spaminfo[ID]-Getspaminfooftheuser.
-lyrics[QUERY]-Getlyricsofthesong.
-paste[TET]-Pastetextonpastebin.
-define[WORD]-GetdefinitionfromDictionary.
-synonyms[WORD]-GetsynonymsfromDictionary.
-antonyms[WORD]-GetantonymsfromDictionary.
-country[QUERY]-GetInformationaboutgivencountry.
-cs-GathersCricketinfo(Globally).
-covid[COUNTRY]-Getcovidupdatesofgivencountry.
-fakegen-Gathersfakeinformation.
-weather[QUERY]-Getweatherinformation.
-datetime[QUERY]-GetDate&timeinformationofgivencountry/region.
-app[QUERY]-Searchforappsinplaystore.
-gh[QUERY]-Searchgithub.
-so[QUERY]-Searchstackoverflow.
-wiki[QUERY]-Searchwikipedia.
-ping-Checkpingrate.
-pokedex[TET]:PokemonSearch
"""

__MODULE__="Inline"
__HELP__="""
==>>**INLINEBOTSERVICEOF@INERUKIBOT**<<==
`I'mmoreefficientwhenaddedasgroupadmin.Bythewaythesecommandscanbeusedbyanyoneinagroupviainline.`

>>Syntax<<
@InerukiBot[command][query]

>>CommandsAvailable<<
-**alive**-__CheckBot'sStats.__
-**yt[query]**-__YoutubeSearch.__
-**tr[LANGUAGE_CODE][QUERY]**-__TranslateText.__
-**ud[QUERY]**-__UrbanDictionaryQuery.__
-**google[QUERY]**-__GoogleSearch.__
-**modapk[name]**-__Giveyoudirectlinkofmodapk__
-**webss[URL]**-__TakeScreenshotOfAWebsite.__
-**bitly[URL]**-__ShortenALink.__
-**wall[Query]**-__FindWallpapers.__
-**pic[Query]**-__Findpictures.__
-**saavn[SONG_NAME]**-__GetSongsFromSaavn.__
-**deezer[SONG_NAME]**-__GetSongsFromDeezer.__
-**torrent[QUERY]**-__TorrentSearch.__
-**reddit[QUERY]**-__Getmemesfromredit.__
-**imdb[QUERY]**-__Searchmoviesonimdb.__
-**spaminfo[id]**-__Getspaminfooftheuser.__
-**lyrics[QUERY]**-__Getlyricsofgivensong.__
-**paste[TET]**-__Pastetextonpastebin.__
-**define[WORD]**-__GetdefinitionfromDictionary.__
-**synonyms[WORD]**-__GetsynonymsfromDictionary.__
-**antonyms[WORD]**-__GetantonymsfromDictionary.__
-**country[QUERY]**-__GetInformationaboutgivencountry.__
-**cs**-__GathersCricketinfo(Globally).__
-**covid[COUNTRY]**-__Getcovidupdatesofgivencountry.__
-**fakegen**-__Gathersfakeinformation.__
-**weather[QUERY]**-__Getweatherinformation.__
-**datetime[QUERY]**-__GetDate&timeinformationofgivencountry/region.__
-**app[QUERY]**-__Searchforappsonplaystore.
-**gh[QUERY]**-__Searchgithub.__
-**so[QUERY]**-__Searchstackoverfolw.__
-**wiki[QUERY]**-__Searchwikipedia.__
-**ping**-__Checkpingrate.__
-**pokedex[TET]**-__PokemonSearch.__
"""


@app.on_message(filters.command("inline"))
asyncdefinline_help(_,message):
awaitapp.send_message(message.chat.id,text=__HELP__)


@app.on_inline_query()
asyncdefinline_query_handler(client,query):
try:
text=query.query.lower()
answers=[]
iftext.strip()=="":
answerss=awaitinline_help_func(__HELP__)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=10)
return
eliftext.split()[0]=="alive":
answerss=awaitalive_function(answers)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=10)
eliftext.split()[0]=="tr":
lang=text.split()[1]
tex=text.split(None,2)[2]
answerss=awaittranslate_func(answers,lang,tex)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=10)
eliftext.split()[0]=="ud":
tex=text.split(None,1)[1]
answerss=awaiturban_func(answers,tex)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=10)
eliftext.split()[0]=="google":
tex=text.split(None,1)[1]
answerss=awaitgoogle_search_func(answers,tex)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=10)
eliftext.split()[0]=="webss":
tex=text.split(None,1)[1]
answerss=awaitwebss(tex)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=2)
eliftext.split()[0]=="bitly":
tex=text.split(None,1)[1]
answerss=awaitshortify(tex)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=2)
eliftext.split()[0]=="wiki":
iflen(text.split())<2:
awaitclient.answer_inline_query(
query.id,
results=answers,
switch_pm_text="Wikipedia|wiki[QUERY]",
switch_pm_parameter="inline",
)
return
tex=text.split(None,1)[1].strip()
answerss=awaitwiki_func(answers,tex)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=2)

eliftext.split()[0]=="ping":
answerss=awaitping_func(answers)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=2)
return

eliftext.split()[0]=="yt":
answers=[]
search_query=text.split(None,1)[1]
search_query=query.query.lower().strip().rstrip()

ifsearch_query=="":
awaitclient.answer_inline_query(
query.id,
results=answers,
switch_pm_text="TypeaYouTubevideoname...",
switch_pm_parameter="help",
cache_time=0,
)
else:
search=VideosSearch(search_query,limit=50)

forresultinsearch.result()["result"]:
answers.append(
InlineQueryResultArticle(
title=result["title"],
description="{},{}views.".format(
result["duration"],result["viewCount"]["short"]
),
input_message_content=InputTextMessageContent(
"https://www.youtube.com/watch?v={}".format(
result["id"]
)
),
thumb_url=result["thumbnails"][0]["url"],
)
)

try:
awaitquery.answer(results=answers,cache_time=0)
excepterrors.QueryIdInvalid:
awaitquery.answer(
results=answers,
cache_time=0,
switch_pm_text="Error:Searchtimedout",
switch_pm_parameter="",
)

eliftext.split()[0]=="wall":
tex=text.split(None,1)[1]
answerss=awaitwall_func(answers,tex)
awaitclient.answer_inline_query(query.id,results=answerss)

eliftext.split()[0]=="pic":
tex=text.split(None,1)[1]
answerss=awaitwall_func(answers,tex)
awaitclient.answer_inline_query(query.id,results=answerss)

eliftext.split()[0]=="saavn":
tex=text.split(None,1)[1]
answerss=awaitsaavn_func(answers,tex)
awaitclient.answer_inline_query(query.id,results=answerss)

eliftext.split()[0]=="deezer":
tex=text.split(None,1)[1]
answerss=awaitdeezer_func(answers,tex)
awaitclient.answer_inline_query(query.id,results=answerss)

eliftext.split()[0]=="torrent":
tex=text.split(None,1)[1]
answerss=awaittorrent_func(answers,tex)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=10)
eliftext.split()[0]=="modapk":
sgname=text.split(None,1)[1]
PabloEscobar=(
f"https://an1.com/tags/MOD/?story={sgname}&do=search&subaction=search"
)
r=requests.get(PabloEscobar)
results=[]
soup=BeautifulSoup(r.content,"html5lib")
mydivs=soup.find_all("div",{"class":"search-results"})
Pop=soup.find_all("div",{"class":"title"})
cnte=len(mydivs)
forcntinrange(cnte):
sucker=mydivs[cnt]
pH9=sucker.find("a").contents[0]
file_name=pH9
pH=sucker.findAll("img")
imme=pH[0]["src"]
Pablo=Pop[0].a["href"]
ro=requests.get(Pablo)
soupe=BeautifulSoup(ro.content,"html5lib")
myopo=soupe.find_all("div",{"class":"item"})
capt=f"**{file_name}**\n**{myopo[0].text}**\n**{myopo[1].text}**\n**{myopo[2].text}**\n**{myopo[3].text}**"
mydis0=soupe.find_all("a",{"class":"get-product"})
Lol9=mydis0[0]
lemk="https://an1.com"+Lol9["href"]
rr=requests.get(lemk)
soup=BeautifulSoup(rr.content,"html5lib")
script=soup.find("script",type="text/javascript")
leek=re.search(r'href=[\'"]?([^\'">]+)',script.text).group()
dl_link=leek[5:]

results.append(
InlineQueryResultPhoto(
photo_url=imme,
title=file_name,
caption=capt,
reply_markup=InlineKeyboardMarkup(
[
[InlineKeyboardButton("DownloadLink",url=lemk)],
[
InlineKeyboardButton(
"DirectDownloadLink",url=dl_link
)
],
]
),
)
)

awaitclient.answer_inline_query(query.id,cache_time=0,results=results)
eliftext.split()[0]=="reddit":
subreddit=text.split(None,1)[1]
results=[]
reddit=awaitarq.reddit(subreddit)
sreddit=reddit.subreddit
title=reddit.title
image=reddit.url
link=reddit.postLink
caption=f"""**Title:**`{title}`
Subreddit:`{sreddit}`"""
results.append(
InlineQueryResultPhoto(
photo_url=image,
title="MemeSearch",
caption=caption,
reply_markup=InlineKeyboardMarkup(
[
[InlineKeyboardButton("PostLink",url=link)],
]
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)

eliftext.split()[0]=="imdb":
movie_name=text.split(None,1)[1]
results=[]
remove_space=movie_name.split("")
final_name="+".join(remove_space)
page=requests.get(
"https://www.imdb.com/find?ref_=nv_sr_fn&q="+final_name+"&s=all"
)
str(page.status_code)
soup=BeautifulSoup(page.content,"lxml")
odds=soup.findAll("tr","odd")
mov_title=odds[0].findNext("td").findNext("td").text
mov_link=(
"http://www.imdb.com/"+odds[0].findNext("td").findNext("td").a["href"]
)
page1=requests.get(mov_link)
soup=BeautifulSoup(page1.content,"lxml")
ifsoup.find("div","poster"):
poster=soup.find("div","poster").img["src"]
else:
poster=""
ifsoup.find("div","title_wrapper"):
pg=soup.find("div","title_wrapper").findNext("div").text
mov_details=re.sub(r"\s+","",pg)
else:
mov_details=""
credits=soup.findAll("div","credit_summary_item")
iflen(credits)==1:
director=credits[0].a.text
writer="Notavailable"
stars="Notavailable"
eliflen(credits)>2:
director=credits[0].a.text
writer=credits[1].a.text
actors=[]
forxincredits[2].findAll("a"):
actors.append(x.text)
actors.pop()
stars=actors[0]+","+actors[1]+","+actors[2]
else:
director=credits[0].a.text
writer="Notavailable"
actors=[]
forxincredits[1].findAll("a"):
actors.append(x.text)
actors.pop()
stars=actors[0]+","+actors[1]+","+actors[2]
ifsoup.find("div","inlinecanwrap"):
story_line=soup.find("div","inlinecanwrap").findAll("p")[0].text
else:
story_line="Notavailable"
info=soup.findAll("div","txt-block")
ifinfo:
mov_country=[]
mov_language=[]
fornodeininfo:
a=node.findAll("a")
foriina:
if"country_of_origin"ini["href"]:
mov_country.append(i.text)
elif"primary_language"ini["href"]:
mov_language.append(i.text)
ifsoup.findAll("div","ratingValue"):
forrinsoup.findAll("div","ratingValue"):
mov_rating=r.strong["title"]
else:
mov_rating="Notavailable"
lol=f"Movie-{mov_title}\nClicktoseemore"
msg=(
"<ahref="+poster+">&#8203;</a>"
"<b>Title:</b><code>"
+mov_title
+"</code>\n<code>"
+mov_details
+"</code>\n<b>Rating:</b><code>"
+mov_rating
+"</code>\n<b>Country:</b><code>"
+mov_country[0]
+"</code>\n<b>Language:</b><code>"
+mov_language[0]
+"</code>\n<b>Director:</b><code>"
+director
+"</code>\n<b>Writer:</b><code>"
+writer
+"</code>\n<b>Stars:</b><code>"
+stars
+"</code>\n<b>IMDBUrl:</b>"
+mov_link
+"\n<b>StoryLine:</b>"
+story_line
)
results.append(
InlineQueryResultArticle(
title="ImdbSearch",
description=lol,
input_message_content=InputTextMessageContent(
msg,disable_web_page_preview=False,parse_mode="HTML"
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)
eliftext.split()[0]=="spaminfo":
cmd=text.split(None,1)[1]
results=[]
url=f"https://api.intellivoid.net/spamprotection/v1/lookup?query={cmd}"
a=awaitAioHttp().get_json(url)
response=a["success"]
ifresponseisTrue:
date=a["results"]["last_updated"]
stats=f"**‚ó¢Intellivoid‚Ä¢SpamProtectionInfo**:\n"
stats+=f'‚Ä¢**Updatedon**:`{datetime.fromtimestamp(date).strftime("%Y-%m-%d%I:%M:%S%p")}`\n'
stats+=f"‚Ä¢**ChatInfo**:[Link](t.me/SpamProtectionBot/?start=00_{cmd})\n"

ifa["results"]["attributes"]["is_potential_spammer"]isTrue:
stats+=f"‚Ä¢**User**:`USERxSPAM`\n"
elifa["results"]["attributes"]["is_operator"]isTrue:
stats+=f"‚Ä¢**User**:`USERxOPERATOR`\n"
elifa["results"]["attributes"]["is_agent"]isTrue:
stats+=f"‚Ä¢**User**:`USERxAGENT`\n"
elifa["results"]["attributes"]["is_whitelisted"]isTrue:
stats+=f"‚Ä¢**User**:`USERxWHITELISTED`\n"

stats+=f'‚Ä¢**Type**:`{a["results"]["entity_type"]}`\n'
stats+=f'‚Ä¢**Language**:`{a["results"]["language_prediction"]["language"]}`\n'
stats+=f'‚Ä¢**LanguageProbability**:`{a["results"]["language_prediction"]["probability"]}`\n'
stats+=f"**SpamPrediction**:\n"
stats+=f'‚Ä¢**HamPrediction**:`{a["results"]["spam_prediction"]["ham_prediction"]}`\n'
stats+=f'‚Ä¢**SpamPrediction**:`{a["results"]["spam_prediction"]["spam_prediction"]}`\n'
stats+=f'**Blacklisted**:`{a["results"]["attributes"]["is_blacklisted"]}`\n'
ifa["results"]["attributes"]["is_blacklisted"]isTrue:
stats+=f'‚Ä¢**Reason**:`{a["results"]["attributes"]["blacklist_reason"]}`\n'
stats+=f'‚Ä¢**Flag**:`{a["results"]["attributes"]["blacklist_flag"]}`\n'
stats+=f'**PTID**:\n`{a["results"]["private_telegram_id"]}`\n'
results.append(
InlineQueryResultArticle(
title="SpamInfo",
description="SearchUsersspaminfo",
input_message_content=InputTextMessageContent(
stats,disable_web_page_preview=True
),
)
)
awaitclient.answer_inline_query(
query.id,cache_time=0,results=results
)
eliftext.split()[0]=="lyrics":
cmd=text.split(None,1)[1]
results=[]

song=""
song=Song.find_song(cmd)
ifsong:
ifsong.lyrics:
reply=song.format()
else:
reply="Couldn'tfindanylyricsforthatsong!trywithartistnamealongwithsongifstilldoesntworktry`.glyrics`"
else:
reply="lyricsnotfound!trywithartistnamealongwithsongifstilldoesntworktry`.glyrics`"

iflen(reply)>4095:
reply="lyricstoobig,Tryusing/lyrics"

results.append(
InlineQueryResultArticle(
title="SongLyrics",
description="Clickheretoseelyrics",
input_message_content=InputTextMessageContent(
reply,disable_web_page_preview=False
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)
eliftext.split()[0]=="pokedex":
iflen(text.split())<2:
awaitclient.answer_inline_query(
query.id,
results=answers,
switch_pm_text="Pokemon[text]",
switch_pm_parameter="pokedex",
)
return
pokedex=text.split(None,1)[1].strip()
Pokedex=awaitpokedexinfo(answers,pokedex)
awaitclient.answer_inline_query(query.id,results=Pokedex,cache_time=2)
eliftext.split()[0]=="paste":
tex=text.split(None,1)[1]
answerss=awaitpaste_func(answers,tex)
awaitclient.answer_inline_query(query.id,results=answerss,cache_time=2)

eliftext.split()[0]=="covid":
lel=text.split(None,1)[1]
results=[]
country=lel.replace("","")
data=awaitfetch(f"https://corona.lmao.ninja/v2/countries/{country}")
data=awaitjson_prettify(data)
results.append(
InlineQueryResultArticle(
title="CovidInfoGatheredsuccesfully",
description=data,
input_message_content=InputTextMessageContent(
data,disable_web_page_preview=False
),
)
)
awaitclient.answer_inline_query(query.id,results=results,cache_time=2)
eliftext.split()[0]=="country":
lel=text.split(None,1)[1]
results=[]
country=CountryInfo(lel)
try:
a=country.info()
except:
a="CountryNotAvaiableCurrently"
name=a.get("name")
bb=a.get("altSpellings")
hu=""
forpinbb:
hu+=p+","

area=a.get("area")
borders=""
hell=a.get("borders")
forfkinhell:
borders+=fk+","

call=""
WhAt=a.get("callingCodes")
forwhatinWhAt:
call+=what+""

capital=a.get("capital")
currencies=""
fker=a.get("currencies")
forFKerinfker:
currencies+=FKer+","

HmM=a.get("demonym")
geo=a.get("geoJSON")
pablo=geo.get("features")
Pablo=pablo[0]
PAblo=Pablo.get("geometry")
EsCoBaR=PAblo.get("type")
iso=""
iSo=a.get("ISO")
forhitleriniSo:
po=iSo.get(hitler)
iso+=po+","
fla=iSo.get("alpha2")
fla.upper()

languages=a.get("languages")
lMAO=""
forlmaoinlanguages:
lMAO+=lmao+","

nonive=a.get("nativeName")
waste=a.get("population")
reg=a.get("region")
sub=a.get("subregion")
tik=a.get("timezones")
tom=""
forjerryintik:
tom+=jerry+","

GOT=a.get("tld")
lanester=""
fortargaryeninGOT:
lanester+=targaryen+","

wiki=a.get("wiki")

caption=f"""<b><u>InformationGatheredSuccessfully</b></u>
<b>
CountryName:-{name}
AlternativeSpellings:-{hu}
CountryArea:-{area}squarekilometers
Borders:-{borders}
CallingCodes:-{call}
Country'sCapital:-{capital}
Country'scurrency:-{currencies}
Demonym:-{HmM}
CountryType:-{EsCoBaR}
ISONames:-{iso}
Languages:-{lMAO}
NativeName:-{nonive}
population:-{waste}
Region:-{reg}
SubRegion:-{sub}
TimeZones:-{tom}
TopLevelDomain:-{lanester}
wikipedia:-{wiki}</b>
GatheredByIneruki.</b>
"""
results.append(
InlineQueryResultArticle(
title=f"Infomationof{name}",
description=f"""
CountryName:-{name}
AlternativeSpellings:-{hu}
CountryArea:-{area}squarekilometers
Borders:-{borders}
CallingCodes:-{call}
Country'sCapital:-{capital}

Touchformoreinfo
""",
input_message_content=InputTextMessageContent(
caption,parse_mode="HTML",disable_web_page_preview=True
),
)
)
awaitclient.answer_inline_query(query.id,results=results,cache_time=2)

eliftext.split()[0]=="fakegen":
results=[]
fake=Faker()
name=str(fake.name())
fake.add_provider(internet)
address=str(fake.address())
ip=fake.ipv4_private()
cc=fake.credit_card_full()
email=fake.ascii_free_email()
job=fake.job()
android=fake.android_platform_token()
pc=fake.chrome()
res=f"<b><u>FakeInformationGenerated</b></u>\n<b>Name:-</b><code>{name}</code>\n\n<b>Address:-</b><code>{address}</code>\n\n<b>IPADDRESS:-</b><code>{ip}</code>\n\n<b>creditcard:-</b><code>{cc}</code>\n\n<b>EmailId:-</b><code>{email}</code>\n\n<b>Job:-</b><code>{job}</code>\n\n<b>androiduseragent:-</b><code>{android}</code>\n\n<b>Pcuseragent:-</b><code>{pc}</code>"
results.append(
InlineQueryResultArticle(
title="Fakeinfomationgathered",
description="Clickheretoseethem",
input_message_content=InputTextMessageContent(
res,parse_mode="HTML",disable_web_page_preview=True
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)

eliftext.split()[0]=="cs":
results=[]
score_page="http://static.cricinfo.com/rss/livescores.xml"
page=urllib.request.urlopen(score_page)
soup=BeautifulSoup(page,"html.parser")
result=soup.find_all("description")
Sed=""
formatchinresult:
Sed+=match.get_text()+"\n\n"
res=f"<b><u>Matchinformationgatheredsuccessful</b></u>\n\n\n<code>{Sed}</code>"
results.append(
InlineQueryResultArticle(
title="Matchinformationgathered",
description="Clickheretoseethem",
input_message_content=InputTextMessageContent(
res,parse_mode="HTML",disable_web_page_preview=False
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)

eliftext.split()[0]=="antonyms":
results=[]
lel=text.split(None,1)[1]
word=f"{lel}"
let=dictionary.antonym(word)
set=str(let)
jet=set.replace("{","")
net=jet.replace("}","")
got=net.replace("'","")
results.append(
InlineQueryResultArticle(
title=f"antonymsfor{lel}",
description=got,
input_message_content=InputTextMessageContent(
got,disable_web_page_preview=False
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)

eliftext.split()[0]=="synonyms":
results=[]
lel=text.split(None,1)[1]
word=f"{lel}"
let=dictionary.synonym(word)
set=str(let)
jet=set.replace("{","")
net=jet.replace("}","")
got=net.replace("'","")
results.append(
InlineQueryResultArticle(
title=f"antonymsfor{lel}",
description=got,
input_message_content=InputTextMessageContent(
got,disable_web_page_preview=False
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)

eliftext.split()[0]=="define":
results=[]
lel=text.split(None,1)[1]
word=f"{lel}"
let=dictionary.meaning(word)
set=str(let)
jet=set.replace("{","")
net=jet.replace("}","")
got=net.replace("'","")
results.append(
InlineQueryResultArticle(
title=f"Definitionfor{lel}",
description=got,
input_message_content=InputTextMessageContent(
got,disable_web_page_preview=False
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)

eliftext.split()[0]=="weather":
results=[]
sample_url="https://api.openweathermap.org/data/2.5/weather?q={}&APPID={}&units=metric"
input_str=text.split(None,1)[1]
asyncwithaiohttp.ClientSession()assession:
response_api_zero=awaitsession.get(
sample_url.format(input_str,OPENWEATHERMAP_ID)
)
response_api=awaitresponse_api_zero.json()
ifresponse_api["cod"]==200:
country_code=response_api["sys"]["country"]
country_time_zone=int(response_api["timezone"])
sun_rise_time=int(response_api["sys"]["sunrise"])+country_time_zone
sun_set_time=int(response_api["sys"]["sunset"])+country_time_zone
lol="""
WEATHERINFOGATHERED
Location:{}
Temperature‚òÄÔ∏è:{}¬∞–°
minimium:{}¬∞–°
maximum:{}¬∞–°
Humidityüå§**:{}%
Windüí®:{}m/s
Clouds‚òÅÔ∏è:{}hpa
Sunriseüå§:{}{}
Sunsetüåù:{}{}""".format(
input_str,
response_api["main"]["temp"],
response_api["main"]["temp_min"],
response_api["main"]["temp_max"],
response_api["main"]["humidity"],
response_api["wind"]["speed"],
response_api["clouds"]["all"],
#response_api["main"]["pressure"],
time.strftime("%Y-%m-%d%H:%M:%S",time.gmtime(sun_rise_time)),
country_code,
time.strftime("%Y-%m-%d%H:%M:%S",time.gmtime(sun_set_time)),
country_code,
)
results.append(
InlineQueryResultArticle(
title=f"WeatherInformation",
description=lol,
input_message_content=InputTextMessageContent(
lol,disable_web_page_preview=True
),
)
)
awaitclient.answer_inline_query(
query.id,cache_time=0,results=results
)

eliftext.split()[0]=="datetime":
results=[]
gay=text.split(None,1)[1]
lel=gay
query_timezone=lel.lower()
iflen(query_timezone)==2:
result=generate_time(query_timezone,["countryCode"])
else:
result=generate_time(query_timezone,["zoneName","countryName"])

ifnotresult:
result=f"Timezoneinfonotavailablefor<b>{lel}</b>"

results.append(
InlineQueryResultArticle(
title=f"Date&Timeinfoof{lel}",
description=result,
input_message_content=InputTextMessageContent(
result,disable_web_page_preview=False,parse_mode="html"
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)

eliftext.split()[0]=="app":
rip=[]
app_name=text.split(None,1)[1]
remove_space=app_name.split("")
final_name="+".join(remove_space)
page=requests.get(
"https://play.google.com/store/search?q="+final_name+"&c=apps"
)
str(page.status_code)
soup=BeautifulSoup(page.content,"lxml",from_encoding="utf-8")
results=soup.findAll("div","ZmHEEd")
app_name=(
results[0]
.findNext("div","Vpfmgd")
.findNext("div","WsMG1cnnK0zc")
.text
)
app_dev=(
results[0].findNext("div","Vpfmgd").findNext("div","KoLSrc").text
)
app_dev_link=(
"https://play.google.com"
+results[0].findNext("div","Vpfmgd").findNext("a","mnKHRc")["href"]
)
app_rating=(
results[0]
.findNext("div","Vpfmgd")
.findNext("div","pf5lIe")
.find("div")["aria-label"]
)
app_link=(
"https://play.google.com"
+results[0]
.findNext("div","Vpfmgd")
.findNext("div","vU6FJp63iDd")
.a["href"]
)
app_icon=(
results[0]
.findNext("div","Vpfmgd")
.findNext("div","uzcko")
.img["data-src"]
)
app_details="<ahref='"+app_icon+"'>üì≤&#8203;</a>"
app_details+="<b>"+app_name+"</b>"
app_details+=(
"\n\n<code>Developer:</code><ahref='"
+app_dev_link
+"'>"
+app_dev
+"</a>"
)
app_details+="\n<code>Rating:</code>"+app_rating.replace(
"Rated","‚≠ê"
).replace("outof","/").replace("stars","",1).replace(
"stars","‚≠ê"
).replace(
"five","5"
)
app_details+=(
"\n<code>Features:</code><ahref='"
+app_link
+"'>ViewinPlayStore</a>"
)
app_details+="\n\n===>@InerukiSupport_Official<==="
rip.append(
InlineQueryResultArticle(
title=f"Datailsof{app_name}",
description=app_details,
input_message_content=InputTextMessageContent(
app_details,disable_web_page_preview=True,parse_mode="html"
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=rip)

eliftext.split()[0]=="gh":
results=[]
gett=text.split(None,1)[1]
text=gett+'"site:github.com"'
gresults=awaitGoogleSearch().async_search(text,1)
result=""
foriinrange(4):
try:
title=gresults["titles"][i].replace("\n","")
source=gresults["links"][i]
description=gresults["descriptions"][i]
result+=f"[{title}]({source})\n"
result+=f"`{description}`\n\n"
exceptIndexError:
pass
results.append(
InlineQueryResultArticle(
title=f"Resultsfor{gett}",
description=f"Githubinfoof{title}\nTouchtoread",
input_message_content=InputTextMessageContent(
result,disable_web_page_preview=True
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)

eliftext.split()[0]=="so":
results=[]
gett=text.split(None,1)[1]
text=gett+'"site:stackoverflow.com"'
gresults=awaitGoogleSearch().async_search(text,1)
result=""
foriinrange(4):
try:
title=gresults["titles"][i].replace("\n","")
source=gresults["links"][i]
description=gresults["descriptions"][i]
result+=f"[{title}]({source})\n"
result+=f"`{description}`\n\n"
exceptIndexError:
pass
results.append(
InlineQueryResultArticle(
title=f"Stackoverflowsaerch-{title}",
description=f"Touchtoviewsearchresultson{title}",
input_message_content=InputTextMessageContent(
result,disable_web_page_preview=True
),
)
)
awaitclient.answer_inline_query(query.id,cache_time=0,results=results)

except(IndexError,TypeError,KeyError,ValueError):
return


defgenerate_time(to_find:str,findtype:List[str])->str:
data=requests.get(
f"http://api.timezonedb.com/v2.1/list-time-zone"
f"?key={TIME_API_KEY}"
f"&format=json"
f"&fields=countryCode,countryName,zoneName,gmtOffset,timestamp,dst"
).json()

forzoneindata["zones"]:
foreachtypeinfindtype:
ifto_findinzone[eachtype].lower():
country_name=zone["countryName"]
country_zone=zone["zoneName"]
country_code=zone["countryCode"]

ifzone["dst"]==1:
daylight_saving="Yes"
else:
daylight_saving="No"

date_fmt=r"%d-%m-%Y"
time_fmt=r"%H:%M:%S"
day_fmt=r"%A"
gmt_offset=zone["gmtOffset"]
timestamp=datetime.datetime.now(
datetime.timezone.utc
)+datetime.timedelta(seconds=gmt_offset)
current_date=timestamp.strftime(date_fmt)
current_time=timestamp.strftime(time_fmt)
current_day=timestamp.strftime(day_fmt)

break

try:
result=(
f"DATEANDTIMEOFCOUNTRY"
f"üåçCountry:{country_name}\n"
f"‚è≥ZoneName:{country_zone}\n"
f"üó∫CountryCode:{country_code}\n"
f"üåûDaylightsaving:{daylight_saving}\n"
f"üåÖDay:{current_day}\n"
f"‚åöCurrentTime:{current_time}\n"
f"üìÜCurrentDate:{current_date}"
)
exceptBaseException:
result=None

returnresult
